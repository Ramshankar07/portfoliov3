<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How I Customized Llama 3.1 8B on a Budget - Ramshankar Bhuvaneswaran</title>
    <link rel="stylesheet" href="../styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome.com/6.0.0/css/all.min.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">RB</div>
            <ul class="nav-menu">
                <li><a href="../index.html" class="nav-link">Home</a></li>
                <li><a href="../about.html" class="nav-link">About</a></li>
                <li><a href="../experience.html" class="nav-link">Experience</a></li>
                <li><a href="../projects.html" class="nav-link">Projects</a></li>
                <li><a href="../blogs.html" class="nav-link active">Blogs</a></li>
                <li><a href="../contact.html" class="nav-link">Contact</a></li>
            </ul>
            <div class="hamburger">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <!-- Blog Post Header -->
    <section class="blog-post-header">
        <div class="container">
            <div class="blog-post-meta">
                <span class="blog-category">AI/ML</span>
                <span class="blog-date">December 25, 2024</span>
                <span class="reading-time">12 min read</span>
            </div>
            <h1 class="blog-post-title">How I Customized Llama 3.1 8B on a Budget</h1>
            <p class="blog-post-subtitle">
                Democratizing AI: Fine-tuning Large Language Models with Limited Resources
            </p>
            <div class="blog-post-tags">
                <span class="blog-tag">LLMs</span>
                <span class="blog-tag">LoRA</span>
                <span class="blog-tag">Fine-tuning</span>
                <span class="blog-tag">Budget AI</span>
                <span class="blog-tag">PyTorch</span>
            </div>
        </div>
    </section>

    <!-- Blog Post Content -->
    <section class="blog-post-content">
        <div class="container">
            <article class="blog-post">
                <div class="blog-post-body">
                    <div class="project-info">
                        <h3>Graduate Student Research</h3>
                        <p><strong>Institution:</strong> Northeastern University</p>
                        <p><strong>Focus:</strong> Budget-Friendly AI</p>
                    </div>

                    <h2>Why I Took on This Challenge</h2>
                    <p>
                        I was impressed by how fine-tuned large language models outperform retrieval-augmented systems, 
                        especially at inference time. So I set out to fine-tune an open-source model like Meta's Llama 3.1 
                        (8B parameters). But most sources stated you needed giant, expensive GPUs and tons of storage 
                        resources that I simply didn't have.
                    </p>
                    
                    <p>
                        Determined to find another way, I tried it on a budget, and discovered you don't need a fancy 
                        setup to customize a state-of-the-art model.
                    </p>

                    <h2>The Hurdles: Time, Memory, and Disk</h2>
                    <p>
                        Fine-tuning an 8 billion-parameter model throws up three big roadblocks:
                    </p>

                    <div class="challenges-grid">
                        <div class="challenge-card">
                            <div class="challenge-icon">
                                <i class="fas fa-microchip"></i>
                            </div>
                            <h4>GPU RAM Required</h4>
                            <div class="challenge-value">16GB</div>
                        </div>
                        <div class="challenge-card">
                            <div class="challenge-icon">
                                <i class="fas fa-clock"></i>
                            </div>
                            <h4>GPU Time Limit</h4>
                            <div class="challenge-value">4hrs</div>
                        </div>
                        <div class="challenge-card">
                            <div class="challenge-icon">
                                <i class="fas fa-hdd"></i>
                            </div>
                            <h4>Disk Storage</h4>
                            <div class="challenge-value">Limited</div>
                        </div>
                    </div>

                    <h2>My Secret Weapon: LoRA (Low-Rank Adaptation)</h2>
                    <p>
                        Instead of retraining 8 billion weights, LoRA tacks on small "adapter" matrices. Think of it 
                        as fine-tuning a car's suspension, rather than redesigning the entire engine.
                    </p>

                    <div class="lora-comparison">
                        <div class="comparison-card full-model">
                            <h4>FULL MODEL</h4>
                            <div class="param-count">8B Parameters</div>
                        </div>
                        <div class="vs-divider">VS</div>
                        <div class="comparison-card lora-model">
                            <h4>LoRA</h4>
                            <div class="param-count">65K Parameters</div>
                            <div class="efficiency">250× FEWER PARAMS</div>
                        </div>
                    </div>

                    <div class="parameter-breakdown">
                        <h4>Parameter Comparison</h4>
                        <ul>
                            <li><strong>Full weight matrix:</strong> 4,096 × 4,096 = 16.7 million parameters</li>
                            <li><strong>LoRA (rank 8):</strong> 4,096 × 8 + 8 × 4,096 = 65,536 parameters</li>
                        </ul>
                        <p class="highlight">That's 250× fewer parameters!</p>
                    </div>

                    <div class="adapter-sizes">
                        <div class="size-card">
                            <h4>Mini (rank 8)</h4>
                            <div class="size-value">81MB</div>
                        </div>
                        <div class="size-card">
                            <h4>Medium (rank 16)</h4>
                            <div class="size-value">161MB</div>
                        </div>
                    </div>

                    <h2>My Setup on Northeastern's Cluster</h2>
                    <p>
                        I tapped into the Discovery cluster, which has:
                    </p>

                    <div class="gpu-specs">
                        <div class="gpu-card h200">
                            <h4>H200</h4>
                            <div class="vram">141GB GPU VRAM</div>
                        </div>
                        <div class="gpu-card a100">
                            <h4>A100</h4>
                            <div class="vram">40GB GPU VRAM</div>
                        </div>
                        <div class="gpu-card v100">
                            <h4>V100</h4>
                            <div class="vram">32GB GPU VRAM</div>
                        </div>
                    </div>

                    <p>
                        Jobs are managed by SLURM, with a hard 4-hour limit. To avoid eating disk quota, I redirected 
                        all caches to scratch space:
                    </p>

                    <div class="code-block">
                        <h4>Cache Configuration</h4>
                        <pre><code>export CACHE_DIR="${SCRATCH}/llama3_finetune/cache/huggingface"
export HF_HOME="$CACHE_DIR"
export TRANSFORMERS_CACHE="$CACHE_DIR"</code></pre>
                    </div>

                    <h2>Organizing the Project</h2>
                    <p>
                        I called it LlamaVox, with a simple structure:
                    </p>

                    <div class="project-structure">
                        <pre><code>LlamaVox/
├── config/     # LoRA settings
├── data/       # Training files
├── models/     # Saved adapters
├── slurm/      # Job scripts
└── src/        # Training code</code></pre>
                    </div>

                    <h3>My Tech Stack:</h3>
                    <ul>
                        <li>PyTorch 2.6.0</li>
                        <li>Transformers 4.53.0</li>
                        <li>PEFT 0.16.0</li>
                        <li>TRL 0.19.0</li>
                        <li>Accelerate</li>
                        <li>bitsandbytes</li>
                    </ul>

                    <h2>Prepping the Data</h2>
                    <p>
                        I built three datasets:
                    </p>

                    <div class="dataset-info">
                        <div class="dataset-card">
                            <h4>Mini Dataset</h4>
                            <div class="dataset-size">5K examples</div>
                            <div class="dataset-file-size">2.2 MB</div>
                        </div>
                        <div class="dataset-card">
                            <h4>Medium Dataset</h4>
                            <div class="dataset-size">50K examples</div>
                        </div>
                        <div class="dataset-card">
                            <h4>Synthetic Dataset</h4>
                            <div class="dataset-size">1K examples</div>
                        </div>
                    </div>

                    <p>
                        Each example looked like this:
                    </p>

                    <div class="code-block">
                        <h4>Data Format</h4>
                        <pre><code>{
  "conversations": [
    {"role": "user", "content": "What are the main challenges of urban planning?"},
    {"role": "assistant", "content": "Urban planning faces several key challenges..."}
  ]
}</code></pre>
                    </div>

                    <h2>Running the Training Jobs</h2>
                    <p>
                        Here's a snippet of my SLURM script for an H200 GPU:
                    </p>

                    <div class="code-block">
                        <h4>SLURM Script for H200</h4>
                        <pre><code>#!/bin/bash
#SBATCH --job-name=llama3_h200
#SBATCH --time=4:00:00
#SBATCH --gres=gpu:h200:1
#SBATCH --mem=96G
#SBATCH --cpus-per-task=16

python src/train.py \
  --model llama-3.1 \
  --dataset data/mini.json \
  --lora_rank 8 \
  --output_dir models/mini_h200</code></pre>
                    </div>

                    <h2>Mini Dataset Results on H200</h2>
                    <div class="training-results">
                        <div class="result-item">
                            <strong>Start:</strong> July 7, 2025 8:06 PM EDT
                        </div>
                        <div class="result-item">
                            <strong>Runtime:</strong> 18 minutes
                        </div>
                        <div class="result-item">
                            <strong>Loss drop:</strong> 0.1164 → 0.0229
                        </div>
                        <div class="result-item">
                            <strong>Token accuracy:</strong> 98.98%
                        </div>
                    </div>

                    <h2>GPU Performance Comparison</h2>
                    <div class="performance-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>GPU</th>
                                    <th>VRAM</th>
                                    <th>Mini (5K)</th>
                                    <th>Medium (50K)</th>
                                    <th>Adapter Size</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>H200</strong></td>
                                    <td>141 GB</td>
                                    <td>18 min</td>
                                    <td>~2.5 hours</td>
                                    <td>81–161 MB</td>
                                </tr>
                                <tr>
                                    <td><strong>A100</strong></td>
                                    <td>40 GB</td>
                                    <td>25 min</td>
                                    <td>~3 hours</td>
                                    <td>81–161 MB</td>
                                </tr>
                                <tr>
                                    <td><strong>V100</strong></td>
                                    <td>32 GB</td>
                                    <td>40 min</td>
                                    <td>Failed</td>
                                    <td>81 MB only</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="gpu-recommendations">
                        <div class="recommendation-card h200">
                            <h4>H200</h4>
                            <p>Blistering speed, but scarce</p>
                        </div>
                        <div class="recommendation-card a100">
                            <h4>A100</h4>
                            <p>Great balance of power & availability</p>
                        </div>
                        <div class="recommendation-card v100">
                            <h4>V100</h4>
                            <p>OK for tiny jobs, hits 4hr wall</p>
                        </div>
                    </div>

                    <h2>Performance Optimization Tricks</h2>
                    <div class="optimization-tricks">
                        <div class="trick-card">
                            <h4>Gradient Accumulation</h4>
                            <p>Fake bigger batches with less memory.</p>
                        </div>
                        <div class="trick-card">
                            <h4>Mixed-Precision Training</h4>
                            <p>Use 16-bit floats to halve memory use.</p>
                        </div>
                        <div class="trick-card">
                            <h4>8-bit Quantization</h4>
                            <p>Load the base model in 8-bit, freeing up VRAM.</p>
                        </div>
                        <div class="trick-card">
                            <h4>Smart Checkpointing</h4>
                            <p>Save every 10 minutes so you can resume if you hit the time limit.</p>
                        </div>
                    </div>

                    <h2>Troubleshooting: Issues & Fixes</h2>
                    <div class="troubleshooting-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Issue</th>
                                    <th>Status</th>
                                    <th>Fix Summary</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Flash Attention Compatibility</td>
                                    <td><span class="status fixed">✅ Fixed</span></td>
                                    <td>Uninstalled flash_attn; set attn_implementation="eager"</td>
                                </tr>
                                <tr>
                                    <td>Disk Quota Exceeded</td>
                                    <td><span class="status fixed">✅ Fixed</span></td>
                                    <td>Redirected HF cache to scratch space</td>
                                </tr>
                                <tr>
                                    <td>Hugging Face Authentication Errors</td>
                                    <td><span class="status fixed">✅ Fixed</span></td>
                                    <td>Added explicit huggingface_hub.login() and token management</td>
                                </tr>
                                <tr>
                                    <td>Environment Setup Complexity</td>
                                    <td><span class="status fixed">✅ Fixed</span></td>
                                    <td>Created run_model.sh for automated setup and GPU checks</td>
                                </tr>
                                <tr>
                                    <td>Out-of-Memory (OOM) Errors</td>
                                    <td><span class="status ongoing">📋 Ongoing</span></td>
                                    <td>8-bit quantization, smaller batches, request more VRAM, monitor</td>
                                </tr>
                                <tr>
                                    <td>Model Access & Permissions</td>
                                    <td><span class="status fixed">✅ Fixed</span></td>
                                    <td>Verified permissions; added access checks before download attempts</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h2>Quick Start Guide</h2>
                    <div class="quick-start">
                        <div class="step">
                            <h4>1. Grab a GPU node</h4>
                            <pre><code>srun --gres=gpu:1 --mem=32G --time=1:00:00 --pty bash</code></pre>
                        </div>
                        <div class="step">
                            <h4>2. Run the launcher</h4>
                            <pre><code>./llama3_finetune/run_model.sh</code></pre>
                        </div>
                    </div>

                    <h2>Key Takeaways</h2>
                    <div class="key-takeaways">
                        <div class="takeaway-card">
                            <h4>🎯 LoRA is Game-Changing</h4>
                            <p>250× parameter reduction makes fine-tuning accessible on consumer hardware.</p>
                        </div>
                        <div class="takeaway-card">
                            <h4>⚡ H200 vs A100 Trade-offs</h4>
                            <p>H200 offers blistering speed but A100 provides better availability and cost-effectiveness.</p>
                        </div>
                        <div class="takeaway-card">
                            <h4>💾 Smart Resource Management</h4>
                            <p>Cache redirection and scratch space usage are crucial for cluster environments.</p>
                        </div>
                        <div class="takeaway-card">
                            <h4>🔧 Optimization is Key</h4>
                            <p>Mixed precision, quantization, and gradient accumulation can make or break your training.</p>
                        </div>
                    </div>

                    <h2>Conclusion</h2>
                    <p>
                        Fine-tuning large language models doesn't have to be expensive or require enterprise-grade hardware. 
                        With LoRA and smart resource management, you can customize state-of-the-art models like Llama 3.1 
                        even on university clusters with strict time and storage limits.
                    </p>

                    <p>
                        The key is understanding your constraints and working within them. Whether you have 4 hours or 40, 
                        16GB or 141GB of VRAM, there's a way to make it work. The democratization of AI isn't just about 
                        open-source models—it's about making the fine-tuning process accessible to everyone.
                    </p>

                    <div class="blog-post-footer">
                        <div class="author-info">
                            <h4>About the Author</h4>
                            <p>
                                Ramshankar Bhuvaneswaran is a Master's student in Information Systems at Northeastern University, 
                                specializing in AI/ML engineering. This research was conducted as part of his graduate studies, 
                                exploring practical approaches to democratizing AI technology.
                            </p>
                        </div>
                        
                        <div class="related-posts">
                            <h4>Related Posts</h4>
                            <ul>
                                <li><a href="llm-guide.html">Understanding Large Language Models: From Theory to Practice</a></li>
                                <li><a href="pytorch-production-optimization.html">Optimizing PyTorch Models for Production Deployment</a></li>
                                <li><a href="aws-sagemaker-pipeline.html">AWS SageMaker: End-to-End ML Pipeline Implementation</a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </article>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 Ramshankar Bhuvaneswaran. All rights reserved.</p>
        </div>
    </footer>

    <script src="../script.js"></script>
    
    <!-- Chatbot Widget -->
    <div class="chatbot-widget" id="chatbotWidget">
        <div class="chatbot-header" id="chatbotHeader">
            <div class="chatbot-title">
                <i class="fas fa-robot"></i>
                <span>Portfolio Assistant</span>
            </div>
            <button class="chatbot-toggle" id="chatbotToggle">
                <i class="fas fa-comments"></i>
            </button>
        </div>
        <div class="chatbot-body" id="chatbotBody">
            <div class="chatbot-messages" id="chatbotMessages">
                <div class="message bot-message">
                    <div class="message-content">
                        <i class="fas fa-robot"></i>
                        <p>Hello! 👋 I'm your AI assistant. Ask me anything about Ramshankar's portfolio, skills, or projects!</p>
                    </div>
                    <div class="message-time">Just now</div>
                </div>
            </div>
            <div class="chatbot-input">
                <input type="text" id="chatbotInput" placeholder="Type your message..." maxlength="200">
                <button id="chatbotSend">
                    <i class="fas fa-paper-plane"></i>
                </button>
            </div>
        </div>
    </div>
</body>
</html>
